{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# debug cuda\n",
    "#import os\n",
    "#os.environ['NUMBA_ENABLE_CUDASIM'] = '1'\n",
    "\n",
    "import numba\n",
    "from numba import SmartArray, cuda, float32, int8, int32\n",
    "from numba.cuda.random import create_xoroshiro128p_states as make_rng_states, xoroshiro128p_uniform_float32 as get_rng\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from scipy.misc import imshow\n",
    "from functools import partial\n",
    "import math\n",
    "from miniutils import progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variations = []\n",
    "\n",
    "class Variation:\n",
    "    def __init__(self, kind, f):\n",
    "        self.kind = kind\n",
    "        self.f = f\n",
    "        self.func_name = f.__name__\n",
    "        self.p = None\n",
    "        \n",
    "    @property\n",
    "    def num_p(self):\n",
    "        return self.kind.num_p\n",
    "        \n",
    "    def __call__(self, p=[]):\n",
    "        assert len(p) <= self.num_p\n",
    "        p = [p[i] if i < len(p) else 0 for i in range(self.num_p)]\n",
    "        return cuda.jit(device=True)(self.f(p))\n",
    "    \n",
    "\n",
    "class VariationKind:\n",
    "    def __init__(self, num_p):\n",
    "        self.num_p = num_p\n",
    "    \n",
    "    def __call__(self, f):\n",
    "        inner_variation_maker = Variation(self, f)\n",
    "        all_variations.append(inner_variation_maker)\n",
    "        return inner_variation_maker\n",
    "    \n",
    "\n",
    "@VariationKind(0)\n",
    "def linear(p=[]):\n",
    "    def f(out_xy, x, y, a, b, c, d, e, f):\n",
    "        out_xy[0] = x\n",
    "        out_xy[1] = y\n",
    "    return f\n",
    "\n",
    "@VariationKind(0)\n",
    "def sinusoidal(p=[]):\n",
    "    def f(out_xy, x, y, a, b, c, d, e, f):\n",
    "        out_xy[0] = math.sin(x)\n",
    "        out_xy[1] = math.sin(y)\n",
    "    return f\n",
    "\n",
    "@VariationKind(0)\n",
    "def spherical(p=[]):\n",
    "    def f(out_xy, x, y, a, b, c, d, e, f):\n",
    "        r = math.sqrt(x**2 + y**2)\n",
    "        r = 1 / r**2\n",
    "        out_xy[0] = r * x\n",
    "        out_xy[1] = r * y\n",
    "    return f\n",
    "\n",
    "@VariationKind(0)\n",
    "def swirl(p=[]):\n",
    "    def f(out_xy, x, y, a, b, c, d, e, f):\n",
    "        r2 = x**2 + y**2\n",
    "        out_xy[0] = x * math.sin(r2) - y * math.cos(r2)\n",
    "        out_xy[1] = x * math.cos(r2) + y * math.sin(r2)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def plot_variation(var, xmn=-1, xmx=1, xrs=41, ymn=-1, ymx=1, yrs=41):\n",
    "    from matplotlib import pyplot as plt\n",
    "    \n",
    "    @cuda.jit\n",
    "    def check(an_array):\n",
    "        thread_id = cuda.grid(1)\n",
    "        pt = cuda.local.array(2, float32)\n",
    "        pt[0] = an_array[thread_id, 0]\n",
    "        pt[1] = an_array[thread_id, 1]\n",
    "        var(pt, pt[0], pt[1], 1, 1, 1, 1, 1, 1)\n",
    "        an_array[thread_id, 0] = pt[0]\n",
    "        an_array[thread_id, 1] = pt[1]\n",
    "    \n",
    "    xs = np.linspace(xmn, xmx, xrs)\n",
    "    ys = np.linspace(ymn, ymx, yrs)\n",
    "    \n",
    "    grid_xs, grid_ys = np.meshgrid(xs, ys)\n",
    "    pts = np.stack([grid_xs.ravel(), grid_ys.ravel()]).T\n",
    "    \n",
    "    check[(len(pts),1), 1](pts)\n",
    "    pts = pts.reshape((xrs, yrs, 2))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    for y in range(yrs):\n",
    "        plt.plot(pts[:,y,0], pts[:,y,1], c='k')\n",
    "    for x in range(xrs):\n",
    "        plt.plot(pts[x,:,0], pts[x,:,1], c='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variation(swirl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()\n",
    "\n",
    "\n",
    "class SimpleTransform:\n",
    "    def __init__(self):\n",
    "        self.linear = np.random.normal(size=(2,3))\n",
    "        \n",
    "    def make_device_function(self):\n",
    "        (a, b, c), (d, e, f) = self.linear.astype('float32')\n",
    "        \n",
    "        @cuda.jit(device=True)\n",
    "        def device_transform(xy):\n",
    "            x = xy[0]\n",
    "            y = xy[1]\n",
    "            xy[0] = a * x + b * y + c\n",
    "            xy[1] = d * x + e * y + f\n",
    "            \n",
    "        return device_transform\n",
    "    \n",
    "\n",
    "\n",
    "class Transform:\n",
    "    def __init__(self):\n",
    "        self.linear = np.random.normal(size=(2,3))\n",
    "        self.nonlinear_weights = softmax(np.random.normal(size=len(all_variations)))\n",
    "        self.variations = [var(*np.random.uniform(size=var.kind.num_p)) for var in all_variations]\n",
    "        self.color = softmax(np.random.uniform(size=3))\n",
    "        self.post_transform = SimpleTransform()\n",
    "        \n",
    "    def make_device_function(self):\n",
    "        import textwrap\n",
    "        \n",
    "        (a, b, c), (d, e, f) = self.linear.astype('float32')\n",
    "        post_transform_kernel = self.post_transform.make_device_function()\n",
    "        nonlinear_weights = self.nonlinear_weights.astype('float32')\n",
    "        variations = tuple(self.variations)\n",
    "        num_vars = len(self.variations)\n",
    "        \n",
    "        run_var = '\\n'.join('\\t' + line for line in textwrap.dedent(\"\"\"\n",
    "        v{j}(variation_cache, lin_x, lin_y, {a}, {b}, {c}, {d}, {e}, {f})\n",
    "        xy[0] += {w} * variation_cache[0]\n",
    "        xy[1] += {w} * variation_cache[1]\n",
    "        \"\"\").strip().split('\\n'))\n",
    "        apply_all_variations = [run_var.format(j=j, \n",
    "            a=a, b=b, c=c, d=d, e=e, f=f, w=self.nonlinear_weights[j])\n",
    "                                for j in range(num_vars)]\n",
    "        apply_all_variations = textwrap.dedent(\"\"\"\n",
    "        def apply_variations(xy, variation_cache, lin_x, lin_y):\n",
    "        {}\n",
    "        \"\"\").strip().format(\"\\n\".join(apply_all_variations))\n",
    "        #print(apply_all_variations)\n",
    "        glbls = dict(\n",
    "            {\"v{}\".format(j): var for j, var in enumerate(variations)}, cuda=cuda\n",
    "        )\n",
    "        #print(apply_all_variations)\n",
    "        exec(apply_all_variations, glbls)\n",
    "        apply_variations = cuda.jit(device=True)(glbls['apply_variations'])\n",
    "        \n",
    "#         for j, var in enumerate(variations):\n",
    "#             exec(\"v{j} = variations[{j}]\".format(j=j))\n",
    "#         @cuda.jit(device=True)\n",
    "#         def apply_variations(xy, variation_cache, lin_x, lin_y):\n",
    "#             v0(variation_cache, lin_x, lin_y, a, b, c, d, e, f)\n",
    "#             xy[0] += nonlinear_weights[0] * variation_cache[0]\n",
    "#             xy[1] += nonlinear_weights[0] * variation_cache[1]\n",
    "#             v1(variation_cache, lin_x, lin_y, a, b, c, d, e, f)\n",
    "#             xy[0] += nonlinear_weights[1] * variation_cache[0]\n",
    "#             xy[1] += nonlinear_weights[1] * variation_cache[1]\n",
    "#             v2(variation_cache, lin_x, lin_y, a, b, c, d, e, f)\n",
    "#             xy[0] += nonlinear_weights[2] * variation_cache[0]\n",
    "#             xy[1] += nonlinear_weights[2] * variation_cache[1]\n",
    "#             v3(variation_cache, lin_x, lin_y, a, b, c, d, e, f)\n",
    "#             xy[0] += nonlinear_weights[3] * variation_cache[0]\n",
    "#             xy[1] += nonlinear_weights[3] * variation_cache[1]\n",
    "        \n",
    "        #assert v0 is variations[0]\n",
    "        assert apply_variations is not None\n",
    "        \n",
    "        @cuda.jit(device=True)\n",
    "        def device_transform(xy):\n",
    "            variation_cache = cuda.local.array(2, float32)\n",
    "            lin_x = a * xy[0] + b * xy[1] + c\n",
    "            lin_y = d * xy[0] + e * xy[1] + f\n",
    "            xy[:] = 0.0\n",
    "            variation_cache[:] = 0.0\n",
    "            #variations[j](variation_cache, lin_x, lin_y, a, b, c, d, e, f)\n",
    "            apply_variations(xy, variation_cache, lin_x, lin_y)\n",
    "            post_transform_kernel(xy)\n",
    "            \n",
    "        return device_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def plot_transform(trans, xmn=-1, xmx=1, xrs=41, ymn=-1, ymx=1, yrs=41):\n",
    "    from matplotlib import pyplot as plt\n",
    "    \n",
    "    trans = trans.make_device_function()\n",
    "    @cuda.jit\n",
    "    def check(an_array):\n",
    "        thread_id = cuda.grid(1)\n",
    "        pt = cuda.local.array(2, float32)\n",
    "        pt[0] = an_array[thread_id, 0]\n",
    "        pt[1] = an_array[thread_id, 1]\n",
    "        trans(pt)\n",
    "        an_array[thread_id, 0] = pt[0]\n",
    "        an_array[thread_id, 1] = pt[1]\n",
    "    \n",
    "    xs = np.linspace(xmn, xmx, xrs)\n",
    "    ys = np.linspace(ymn, ymx, yrs)\n",
    "    \n",
    "    grid_xs, grid_ys = np.meshgrid(xs, ys)\n",
    "    pts = np.stack([grid_xs.ravel(), grid_ys.ravel()]).T\n",
    "    \n",
    "    check[(len(pts),1), 1](pts)\n",
    "    pts = pts.reshape((xrs, yrs, 2))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    for y in range(yrs):\n",
    "        plt.plot(pts[:,y,0], pts[:,y,1], c='k')\n",
    "    for x in range(xrs):\n",
    "        plt.plot(pts[x,:,0], pts[x,:,1], c='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "plot_transform(Transform(), xrs=101, yrs=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kernel(num_points, num_steps, transforms, transition_matrix, bounds, resolution, min_step=20, final_transform=None, final_color=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param num_points: Number of points to execute in lock-step in kernel\n",
    "    :param num_steps: Number of timesteps per point\n",
    "    :param transforms: [Transform]\n",
    "    :param transition_matrix: Matrix of transition probabilities from transform i to j\n",
    "    :param bounds: ((xmin, xmax),(ymin,y_max))\n",
    "    :param resolution: (xres, yres)\n",
    "    :param min_step: The first step to actually plot a transformed point\n",
    "    :returns: A cuda function that will execute the desired function\n",
    "    :returns: A function that returns the output image that the cuda function writes to\n",
    "    \"\"\"\n",
    "    \n",
    "    ((xmin, xmax),(ymin, ymax)) = bounds\n",
    "    xptp = xmax - xmin\n",
    "    yptp = ymax - ymin\n",
    "    (xres, yres) = resolution\n",
    "    num_colors = 4\n",
    "    _r = 0; _g = 1; _b = 2; _a = 3\n",
    "    num_transforms = len(transforms)\n",
    "    \n",
    "    #cuda_output_image = SmartArray(shape=(num_colors, yres, xres), dtype='float32', where='gpu')\n",
    "    cuda_output_image = cuda.to_device(np.zeros((num_colors, yres, xres), dtype='float32'))\n",
    "    transition_matrix = cuda.to_device(transition_matrix)\n",
    "    return_image = cuda_output_image.copy_to_host\n",
    "    \n",
    "    transform_functions = [trans.make_device_function() for trans in transforms]\n",
    "    transform_colors = cuda.to_device(np.array([trans.color for trans in transforms]))\n",
    "    \n",
    "    if final_transform is None:\n",
    "        final_transform = SimpleTransform()\n",
    "        final_transform.linear = np.array([[1,0,0],[0,1,0]], dtype='float32')\n",
    "        final_transform = final_transform.make_device_function()\n",
    "        \n",
    "    if final_color is None:\n",
    "        final_color = (1.0, 1.0, 1.0)\n",
    "    final_r, final_g, final_b = final_color\n",
    "    \n",
    "    def runner(blocks):\n",
    "        @cuda.jit(device=True)\n",
    "        def pick_next_transform(current, thread_id, rng_states, transition_matrix):\n",
    "            rng = get_rng(rng_states, thread_id)\n",
    "            for i in range(num_transforms):\n",
    "                if rng <= transition_matrix[current, i]:\n",
    "                    return i\n",
    "                rng -= transition_matrix[current, i]\n",
    "            return 0\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def hist(val, vmin, vptp, vres):\n",
    "            return int32(((val - vmin) / vptp) * vres)\n",
    "\n",
    "#         @cuda.jit(device=True)\n",
    "#         def idx(col, ybin, xbin):\n",
    "#             return col * (xres * yres) + ybin * xres + xbin\n",
    "        \n",
    "        dct = {'t{}'.format(i): transform_functions[i] for i in range(len(transforms))}\n",
    "        code = '''\n",
    "def call_transform(i, pt):\n",
    "    if False:\n",
    "        pass\n",
    "{}\n",
    "'''.format('\\n'.join('''\n",
    "    elif i == {i}:\n",
    "         t{i}(pt)\n",
    "'''.strip('\\n').format(i=i) for i in range(len(transforms)))\n",
    "        )\n",
    "        exec(code, dct)\n",
    "        call_transform = cuda.jit(device=True)(dct['call_transform'])\n",
    "\n",
    "        @cuda.jit\n",
    "        def kernel(transform_colors, rng_states, transition_matrix, out):\n",
    "            thread_id = cuda.grid(1)\n",
    "            #if thread_id == 0:\n",
    "            #    from pdb import set_trace; set_trace()\n",
    "\n",
    "            transform_ids = cuda.shared.array(num_steps, dtype=int8)\n",
    "            transform_ids[0] = 0\n",
    "            if cuda.threadIdx.x == 0:\n",
    "                for i in range(1, num_steps):\n",
    "                    transform_ids[i] = pick_next_transform(transform_ids[i-1], thread_id, rng_states, transition_matrix)\n",
    "            cuda.syncthreads()\n",
    "                    \n",
    "            pt = cuda.local.array(2, float32)\n",
    "            pt[0] = get_rng(rng_states, thread_id) * 2 - 1\n",
    "            pt[1] = get_rng(rng_states, thread_id) * 2 - 1\n",
    "            r = g = b = 1.0\n",
    "            final_pt = cuda.local.array(2, float32)\n",
    "\n",
    "            for i in range(num_steps):\n",
    "                call_transform(i, pt)\n",
    "                r = (r + transform_colors[i,_r]) / 2\n",
    "                g = (g + transform_colors[i,_g]) / 2\n",
    "                b = (b + transform_colors[i,_b]) / 2\n",
    "\n",
    "                if i >= min_step:\n",
    "                    final_pt[0] = pt[0]\n",
    "                    final_pt[1] = pt[1]\n",
    "                    final_transform(final_pt)\n",
    "                    rf = (r + final_r) / 2\n",
    "                    gf = (g + final_g) / 2\n",
    "                    bf = (b + final_b) / 2\n",
    "\n",
    "                    xbin = hist(final_pt[0], xmin, xptp, xres)\n",
    "                    ybin = hist(final_pt[1], ymin, yptp, yres)\n",
    "                    cuda.atomic.add(out, (_r, ybin, xbin), rf)\n",
    "                    cuda.atomic.add(out, (_g, ybin, xbin), gf)\n",
    "                    cuda.atomic.add(out, (_b, ybin, xbin), bf)\n",
    "                    cuda.atomic.add(out, (_a, ybin, xbin), 1.0)\n",
    "                    \n",
    "        threads_per_block = num_points\n",
    "        rng_states = make_rng_states(threads_per_block * blocks, seed=0)\n",
    "        kernel[(blocks,), threads_per_block](transform_colors, rng_states, transition_matrix, cuda_output_image)\n",
    "                \n",
    "    return runner, return_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "transforms = [Transform() for _ in range(10)]\n",
    "transition_matrix = np.random.normal(size=(10,10))\n",
    "transition_matrix /= np.sum(transition_matrix, axis=1, keepdims=True)\n",
    "kernel, get_img = make_kernel(10, 10000, transforms, transition_matrix, ((-5,5),(-5,5)), (128, 128), min_step=20)\n",
    "\n",
    "plt.figure()\n",
    "for i in progbar(100):\n",
    "    kernel(1000)\n",
    "    r, g, b, a = get_img()\n",
    "    #print(np.sum(a))\n",
    "    im = np.stack([r,g,b], axis=-1)\n",
    "    #print(im.shape)\n",
    "    plt.imshow(np.log1p(im), interpolation='nearest')\n",
    "    plt.show()\n",
    "    plt.gcf().canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def increment_by_one(an_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < an_array.size:\n",
    "        an_array[pos] += 1\n",
    "        \n",
    "        \n",
    "an_array = np.arange(16 * 16 * 2)\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid = len(an_array) // (np.prod(threadsperblock))\n",
    "increment_by_one[(blockspergrid,1), threadsperblock](an_array)\n",
    "print(an_array[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def ones(arr):\n",
    "    arr[0] = 1.0\n",
    "    arr[1] = 1.0\n",
    "\n",
    "@cuda.jit\n",
    "def do_something(an_array):\n",
    "    pos = cuda.grid(1)\n",
    "    cache = cuda.local.array(2, float32)\n",
    "    ones(cache)\n",
    "    an_array[pos] += cache[0] + cache[1]\n",
    "\n",
    "\n",
    "an_array = np.arange(16 * 16 * 2, dtype='float32')\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid = len(an_array) // (np.prod(threadsperblock))\n",
    "do_something[(blockspergrid,1), threadsperblock](an_array)\n",
    "print(an_array[0:5])\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit\n",
    "def compute_pi(rng_states, iterations, out):\n",
    "    \"\"\"Find the maximum value in values and store in result[0]\"\"\"\n",
    "    thread_id = cuda.grid(1)\n",
    "\n",
    "    # Compute pi by drawing random (x, y) points and finding what\n",
    "    # fraction lie inside a unit circle\n",
    "    inside = 0\n",
    "    for i in range(iterations):\n",
    "        x = xoroshiro128p_uniform_float32(rng_states, thread_id)\n",
    "        y = xoroshiro128p_uniform_float32(rng_states, thread_id)\n",
    "        if x**2 + y**2 <= 1.0:\n",
    "            inside += 1\n",
    "\n",
    "    out[thread_id] = 4.0 * inside / iterations\n",
    "\n",
    "threads_per_block = 64\n",
    "blocks = 24\n",
    "rng_states = create_xoroshiro128p_states(threads_per_block * blocks, seed=1)\n",
    "out = np.zeros(threads_per_block * blocks, dtype=np.float32)\n",
    "\n",
    "compute_pi[blocks, threads_per_block](rng_states, 10000, out)\n",
    "print('pi:', out.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
